{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axfq5eHXpB-I",
        "outputId": "f8e556de-f4af-4555-9baf-63807e67d8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch : 1 / 350, cost : 2.349175214767456\n",
            "Epoch : 2 / 350, cost : 2.1999223232269287\n",
            "Epoch : 3 / 350, cost : 2.093247890472412\n",
            "Epoch : 4 / 350, cost : 2.056260108947754\n",
            "Epoch : 5 / 350, cost : 2.0174760818481445\n",
            "Epoch : 6 / 350, cost : 1.9705772399902344\n",
            "Epoch : 7 / 350, cost : 1.927689552307129\n",
            "Epoch : 8 / 350, cost : 1.8961387872695923\n",
            "Epoch : 9 / 350, cost : 1.867374300956726\n",
            "Epoch : 10 / 350, cost : 1.841262698173523\n",
            "Epoch : 11 / 350, cost : 1.818313479423523\n",
            "Epoch : 12 / 350, cost : 1.7959706783294678\n",
            "Epoch : 13 / 350, cost : 1.7774440050125122\n",
            "Epoch : 14 / 350, cost : 1.7613338232040405\n",
            "Epoch : 15 / 350, cost : 1.7407426834106445\n",
            "Epoch : 16 / 350, cost : 1.7234339714050293\n",
            "Epoch : 17 / 350, cost : 1.7046362161636353\n",
            "Epoch : 18 / 350, cost : 1.6868185997009277\n",
            "Epoch : 19 / 350, cost : 1.672501802444458\n",
            "Epoch : 20 / 350, cost : 1.6529356241226196\n",
            "Epoch : 21 / 350, cost : 1.6404573917388916\n",
            "Epoch : 22 / 350, cost : 1.630068063735962\n",
            "Epoch : 23 / 350, cost : 1.6139116287231445\n",
            "Epoch : 24 / 350, cost : 1.602504849433899\n",
            "Epoch : 25 / 350, cost : 1.5891779661178589\n",
            "Epoch : 26 / 350, cost : 1.5790421962738037\n",
            "Epoch : 27 / 350, cost : 1.563496470451355\n",
            "Epoch : 28 / 350, cost : 1.5492587089538574\n",
            "Epoch : 29 / 350, cost : 1.5414036512374878\n",
            "Epoch : 30 / 350, cost : 1.5301764011383057\n",
            "Epoch : 31 / 350, cost : 1.5258991718292236\n",
            "Epoch : 32 / 350, cost : 1.5183297395706177\n",
            "Epoch : 33 / 350, cost : 1.5043904781341553\n",
            "Epoch : 34 / 350, cost : 1.4979287385940552\n",
            "Epoch : 35 / 350, cost : 1.4905778169631958\n",
            "Epoch : 36 / 350, cost : 1.4837087392807007\n",
            "Epoch : 37 / 350, cost : 1.474473237991333\n",
            "Epoch : 38 / 350, cost : 1.4695343971252441\n",
            "Epoch : 39 / 350, cost : 1.46396803855896\n",
            "Epoch : 40 / 350, cost : 1.461157202720642\n",
            "Epoch : 41 / 350, cost : 1.4538395404815674\n",
            "Epoch : 42 / 350, cost : 1.443927526473999\n",
            "Epoch : 43 / 350, cost : 1.4319477081298828\n",
            "Epoch : 44 / 350, cost : 1.4259825944900513\n",
            "Epoch : 45 / 350, cost : 1.418030023574829\n",
            "Epoch : 46 / 350, cost : 1.4160724878311157\n",
            "Epoch : 47 / 350, cost : 1.4043693542480469\n",
            "Epoch : 48 / 350, cost : 1.3972125053405762\n",
            "Epoch : 49 / 350, cost : 1.3904134035110474\n",
            "Epoch : 50 / 350, cost : 1.3866136074066162\n",
            "Epoch : 51 / 350, cost : 1.3829962015151978\n",
            "Epoch : 52 / 350, cost : 1.3739070892333984\n",
            "Epoch : 53 / 350, cost : 1.3662863969802856\n",
            "Epoch : 54 / 350, cost : 1.3598902225494385\n",
            "Epoch : 55 / 350, cost : 1.362752914428711\n",
            "Epoch : 56 / 350, cost : 1.357232928276062\n",
            "Epoch : 57 / 350, cost : 1.349780797958374\n",
            "Epoch : 58 / 350, cost : 1.3441756963729858\n",
            "Epoch : 59 / 350, cost : 1.3393124341964722\n",
            "Epoch : 60 / 350, cost : 1.33456552028656\n",
            "Epoch : 61 / 350, cost : 1.3266385793685913\n",
            "Epoch : 62 / 350, cost : 1.3243147134780884\n",
            "Epoch : 63 / 350, cost : 1.3155131340026855\n",
            "Epoch : 64 / 350, cost : 1.3142539262771606\n",
            "Epoch : 65 / 350, cost : 1.3092355728149414\n",
            "Epoch : 66 / 350, cost : 1.3070580959320068\n",
            "Epoch : 67 / 350, cost : 1.298569679260254\n",
            "Epoch : 68 / 350, cost : 1.2966808080673218\n",
            "Epoch : 69 / 350, cost : 1.2965240478515625\n",
            "Epoch : 70 / 350, cost : 1.2831242084503174\n",
            "Epoch : 71 / 350, cost : 1.2953354120254517\n",
            "Epoch : 72 / 350, cost : 1.285029411315918\n",
            "Epoch : 73 / 350, cost : 1.2806507349014282\n",
            "Epoch : 74 / 350, cost : 1.2746058702468872\n",
            "Epoch : 75 / 350, cost : 1.2662487030029297\n",
            "Epoch : 76 / 350, cost : 1.261499285697937\n",
            "Epoch : 77 / 350, cost : 1.2535605430603027\n",
            "Epoch : 78 / 350, cost : 1.2531732320785522\n",
            "Epoch : 79 / 350, cost : 1.2468276023864746\n",
            "Epoch : 80 / 350, cost : 1.2422891855239868\n",
            "Epoch : 81 / 350, cost : 1.2379497289657593\n",
            "Epoch : 82 / 350, cost : 1.2382694482803345\n",
            "Epoch : 83 / 350, cost : 1.2310909032821655\n",
            "Epoch : 84 / 350, cost : 1.223960518836975\n",
            "Epoch : 85 / 350, cost : 1.2178152799606323\n",
            "Epoch : 86 / 350, cost : 1.2119804620742798\n",
            "Epoch : 87 / 350, cost : 1.2133758068084717\n",
            "Epoch : 88 / 350, cost : 1.2089917659759521\n",
            "Epoch : 89 / 350, cost : 1.2060545682907104\n",
            "Epoch : 90 / 350, cost : 1.1956188678741455\n",
            "Epoch : 91 / 350, cost : 1.1936204433441162\n",
            "Epoch : 92 / 350, cost : 1.1956532001495361\n",
            "Epoch : 93 / 350, cost : 1.1904101371765137\n",
            "Epoch : 94 / 350, cost : 1.184065818786621\n",
            "Epoch : 95 / 350, cost : 1.1779937744140625\n",
            "Epoch : 96 / 350, cost : 1.179347276687622\n",
            "Epoch : 97 / 350, cost : 1.172383427619934\n",
            "Epoch : 98 / 350, cost : 1.1672420501708984\n",
            "Epoch : 99 / 350, cost : 1.169580101966858\n",
            "Epoch : 100 / 350, cost : 1.1621850728988647\n",
            "Epoch : 101 / 350, cost : 1.1515265703201294\n",
            "Epoch : 102 / 350, cost : 1.151553988456726\n",
            "Epoch : 103 / 350, cost : 1.1485168933868408\n",
            "Epoch : 104 / 350, cost : 1.1450732946395874\n",
            "Epoch : 105 / 350, cost : 1.1353222131729126\n",
            "Epoch : 106 / 350, cost : 1.1390018463134766\n",
            "Epoch : 107 / 350, cost : 1.134832739830017\n",
            "Epoch : 108 / 350, cost : 1.1291756629943848\n",
            "Epoch : 109 / 350, cost : 1.1296762228012085\n",
            "Epoch : 110 / 350, cost : 1.1232575178146362\n",
            "Epoch : 111 / 350, cost : 1.1206928491592407\n",
            "Epoch : 112 / 350, cost : 1.120888590812683\n",
            "Epoch : 113 / 350, cost : 1.1138465404510498\n",
            "Epoch : 114 / 350, cost : 1.1125648021697998\n",
            "Epoch : 115 / 350, cost : 1.1052701473236084\n",
            "Epoch : 116 / 350, cost : 1.1040586233139038\n",
            "Epoch : 117 / 350, cost : 1.099820852279663\n",
            "Epoch : 118 / 350, cost : 1.09209144115448\n",
            "Epoch : 119 / 350, cost : 1.0967015027999878\n",
            "Epoch : 120 / 350, cost : 1.092140793800354\n",
            "Epoch : 121 / 350, cost : 1.0855069160461426\n",
            "Epoch : 122 / 350, cost : 1.0880911350250244\n",
            "Epoch : 123 / 350, cost : 1.0892199277877808\n",
            "Epoch : 124 / 350, cost : 1.0784962177276611\n",
            "Epoch : 125 / 350, cost : 1.0767853260040283\n",
            "Epoch : 126 / 350, cost : 1.0719143152236938\n",
            "Epoch : 127 / 350, cost : 1.070064902305603\n",
            "Epoch : 128 / 350, cost : 1.064460039138794\n",
            "Epoch : 129 / 350, cost : 1.0660109519958496\n",
            "Epoch : 130 / 350, cost : 1.0599589347839355\n",
            "Epoch : 131 / 350, cost : 1.0557390451431274\n",
            "Epoch : 132 / 350, cost : 1.047775149345398\n",
            "Epoch : 133 / 350, cost : 1.053184151649475\n",
            "Epoch : 134 / 350, cost : 1.0479044914245605\n",
            "Epoch : 135 / 350, cost : 1.044735312461853\n",
            "Epoch : 136 / 350, cost : 1.0455329418182373\n",
            "Epoch : 137 / 350, cost : 1.0388325452804565\n",
            "Epoch : 138 / 350, cost : 1.0366168022155762\n",
            "Epoch : 139 / 350, cost : 1.0320711135864258\n",
            "Epoch : 140 / 350, cost : 1.0344911813735962\n",
            "Epoch : 141 / 350, cost : 1.0328936576843262\n",
            "Epoch : 142 / 350, cost : 1.02950119972229\n",
            "Epoch : 143 / 350, cost : 1.0238226652145386\n",
            "Epoch : 144 / 350, cost : 1.018680453300476\n",
            "Epoch : 145 / 350, cost : 1.0182433128356934\n",
            "Epoch : 146 / 350, cost : 1.006947636604309\n",
            "Epoch : 147 / 350, cost : 1.0049645900726318\n",
            "Epoch : 148 / 350, cost : 1.0010724067687988\n",
            "Epoch : 149 / 350, cost : 1.0004533529281616\n",
            "Epoch : 150 / 350, cost : 0.9984172582626343\n",
            "Epoch : 151 / 350, cost : 0.9951932430267334\n",
            "Epoch : 152 / 350, cost : 0.9908169507980347\n",
            "Epoch : 153 / 350, cost : 0.9892419576644897\n",
            "Epoch : 154 / 350, cost : 0.9853554368019104\n",
            "Epoch : 155 / 350, cost : 0.9856716990470886\n",
            "Epoch : 156 / 350, cost : 0.9764938354492188\n",
            "Epoch : 157 / 350, cost : 0.9727817177772522\n",
            "Epoch : 158 / 350, cost : 0.9756278991699219\n",
            "Epoch : 159 / 350, cost : 0.9693593978881836\n",
            "Epoch : 160 / 350, cost : 0.9730980396270752\n",
            "Epoch : 161 / 350, cost : 0.9616900086402893\n",
            "Epoch : 162 / 350, cost : 0.9616863131523132\n",
            "Epoch : 163 / 350, cost : 0.9578850865364075\n",
            "Epoch : 164 / 350, cost : 0.9524694681167603\n",
            "Epoch : 165 / 350, cost : 0.952779233455658\n",
            "Epoch : 166 / 350, cost : 0.9479485154151917\n",
            "Epoch : 167 / 350, cost : 0.9453158378601074\n",
            "Epoch : 168 / 350, cost : 0.9456945061683655\n",
            "Epoch : 169 / 350, cost : 0.943386435508728\n",
            "Epoch : 170 / 350, cost : 0.9395389556884766\n",
            "Epoch : 171 / 350, cost : 0.9349253177642822\n",
            "Epoch : 172 / 350, cost : 0.9341872930526733\n",
            "Epoch : 173 / 350, cost : 0.9330068230628967\n",
            "Epoch : 174 / 350, cost : 0.928598165512085\n",
            "Epoch : 175 / 350, cost : 0.925265371799469\n",
            "Epoch : 176 / 350, cost : 0.923335611820221\n",
            "Epoch : 177 / 350, cost : 0.9208793640136719\n",
            "Epoch : 178 / 350, cost : 0.9249635338783264\n",
            "Epoch : 179 / 350, cost : 0.9195320010185242\n",
            "Epoch : 180 / 350, cost : 0.9203563928604126\n",
            "Epoch : 181 / 350, cost : 0.9144015908241272\n",
            "Epoch : 182 / 350, cost : 0.917606770992279\n",
            "Epoch : 183 / 350, cost : 0.9390503168106079\n",
            "Epoch : 184 / 350, cost : 0.9502450227737427\n",
            "Epoch : 185 / 350, cost : 0.9333242177963257\n",
            "Epoch : 186 / 350, cost : 0.921414315700531\n",
            "Epoch : 187 / 350, cost : 0.9201127290725708\n",
            "Epoch : 188 / 350, cost : 0.9219751954078674\n",
            "Epoch : 189 / 350, cost : 0.9159355759620667\n",
            "Epoch : 190 / 350, cost : 0.9155162572860718\n",
            "Epoch : 191 / 350, cost : 0.8926464915275574\n",
            "Epoch : 192 / 350, cost : 0.8866423964500427\n",
            "Epoch : 193 / 350, cost : 0.8849232792854309\n",
            "Epoch : 194 / 350, cost : 0.8845992088317871\n",
            "Epoch : 195 / 350, cost : 0.8841202259063721\n",
            "Epoch : 196 / 350, cost : 0.8833245635032654\n",
            "Epoch : 197 / 350, cost : 0.8751468062400818\n",
            "Epoch : 198 / 350, cost : 0.8755699396133423\n",
            "Epoch : 199 / 350, cost : 0.8714424967765808\n",
            "Epoch : 200 / 350, cost : 0.866769015789032\n",
            "Epoch : 201 / 350, cost : 0.8666448593139648\n",
            "Epoch : 202 / 350, cost : 0.8582812547683716\n",
            "Epoch : 203 / 350, cost : 0.8597483038902283\n",
            "Epoch : 204 / 350, cost : 0.852990448474884\n",
            "Epoch : 205 / 350, cost : 0.861303985118866\n",
            "Epoch : 206 / 350, cost : 0.8604178428649902\n",
            "Epoch : 207 / 350, cost : 0.853184700012207\n",
            "Epoch : 208 / 350, cost : 0.8535334467887878\n",
            "Epoch : 209 / 350, cost : 0.8548474907875061\n",
            "Epoch : 210 / 350, cost : 0.8630496263504028\n",
            "Epoch : 211 / 350, cost : 0.8860779404640198\n",
            "Epoch : 212 / 350, cost : 0.8839681148529053\n",
            "Epoch : 213 / 350, cost : 0.8607065677642822\n",
            "Epoch : 214 / 350, cost : 0.8470092415809631\n",
            "Epoch : 215 / 350, cost : 0.844912052154541\n",
            "Epoch : 216 / 350, cost : 0.8435553908348083\n",
            "Epoch : 217 / 350, cost : 0.8336696028709412\n",
            "Epoch : 218 / 350, cost : 0.8296757340431213\n",
            "Epoch : 219 / 350, cost : 0.8242802619934082\n",
            "Epoch : 220 / 350, cost : 0.8254588842391968\n",
            "Epoch : 221 / 350, cost : 0.8207241892814636\n",
            "Epoch : 222 / 350, cost : 0.8146738409996033\n",
            "Epoch : 223 / 350, cost : 0.8117162585258484\n",
            "Epoch : 224 / 350, cost : 0.8071451783180237\n",
            "Epoch : 225 / 350, cost : 0.8143877387046814\n",
            "Epoch : 226 / 350, cost : 0.8063079118728638\n",
            "Epoch : 227 / 350, cost : 0.8004099130630493\n",
            "Epoch : 228 / 350, cost : 0.7972502708435059\n",
            "Epoch : 229 / 350, cost : 0.7957108616828918\n",
            "Epoch : 230 / 350, cost : 0.7987027764320374\n",
            "Epoch : 231 / 350, cost : 0.7881327867507935\n",
            "Epoch : 232 / 350, cost : 0.7876805067062378\n",
            "Epoch : 233 / 350, cost : 0.7822439670562744\n",
            "Epoch : 234 / 350, cost : 0.7777447700500488\n",
            "Epoch : 235 / 350, cost : 0.7803434729576111\n",
            "Epoch : 236 / 350, cost : 0.7797495126724243\n",
            "Epoch : 237 / 350, cost : 0.7743561267852783\n",
            "Epoch : 238 / 350, cost : 0.7734518051147461\n",
            "Epoch : 239 / 350, cost : 0.7657352685928345\n",
            "Epoch : 240 / 350, cost : 0.7660475969314575\n",
            "Epoch : 241 / 350, cost : 0.7671133279800415\n",
            "Epoch : 242 / 350, cost : 0.7641568779945374\n",
            "Epoch : 243 / 350, cost : 0.7576805353164673\n",
            "Epoch : 244 / 350, cost : 0.7605336308479309\n",
            "Epoch : 245 / 350, cost : 0.7603536248207092\n",
            "Epoch : 246 / 350, cost : 0.7545601725578308\n",
            "Epoch : 247 / 350, cost : 0.7561070322990417\n",
            "Epoch : 248 / 350, cost : 0.7542738318443298\n",
            "Epoch : 249 / 350, cost : 0.7551431059837341\n",
            "Epoch : 250 / 350, cost : 0.7473509907722473\n",
            "Epoch : 251 / 350, cost : 0.7458438873291016\n",
            "Epoch : 252 / 350, cost : 0.7479620575904846\n",
            "Epoch : 253 / 350, cost : 0.7487248778343201\n",
            "Epoch : 254 / 350, cost : 0.7454153299331665\n",
            "Epoch : 255 / 350, cost : 0.7430800199508667\n",
            "Epoch : 256 / 350, cost : 0.7368128895759583\n",
            "Epoch : 257 / 350, cost : 0.7325171232223511\n",
            "Epoch : 258 / 350, cost : 0.7303280830383301\n",
            "Epoch : 259 / 350, cost : 0.7369402647018433\n",
            "Epoch : 260 / 350, cost : 0.7247810363769531\n",
            "Epoch : 261 / 350, cost : 0.7258294224739075\n",
            "Epoch : 262 / 350, cost : 0.7277042269706726\n",
            "Epoch : 263 / 350, cost : 0.7214760780334473\n",
            "Epoch : 264 / 350, cost : 0.718146800994873\n",
            "Epoch : 265 / 350, cost : 0.7203803658485413\n",
            "Epoch : 266 / 350, cost : 0.7096806764602661\n",
            "Epoch : 267 / 350, cost : 0.7103388905525208\n",
            "Epoch : 268 / 350, cost : 0.7159066796302795\n",
            "Epoch : 269 / 350, cost : 0.7087499499320984\n",
            "Epoch : 270 / 350, cost : 0.7096326351165771\n",
            "Epoch : 271 / 350, cost : 0.7099140286445618\n",
            "Epoch : 272 / 350, cost : 0.7103996872901917\n",
            "Epoch : 273 / 350, cost : 0.710488498210907\n",
            "Epoch : 274 / 350, cost : 0.704829216003418\n",
            "Epoch : 275 / 350, cost : 0.706671953201294\n",
            "Epoch : 276 / 350, cost : 0.701592743396759\n",
            "Epoch : 277 / 350, cost : 0.6963560581207275\n",
            "Epoch : 278 / 350, cost : 0.6963253021240234\n",
            "Epoch : 279 / 350, cost : 0.6957144141197205\n",
            "Epoch : 280 / 350, cost : 0.692344605922699\n",
            "Epoch : 281 / 350, cost : 0.6953147053718567\n",
            "Epoch : 282 / 350, cost : 0.6948280334472656\n",
            "Epoch : 283 / 350, cost : 0.694813072681427\n",
            "Epoch : 284 / 350, cost : 0.6969699859619141\n",
            "Epoch : 285 / 350, cost : 0.6939818263053894\n",
            "Epoch : 286 / 350, cost : 0.6872729659080505\n",
            "Epoch : 287 / 350, cost : 0.677405834197998\n",
            "Epoch : 288 / 350, cost : 0.6793899536132812\n",
            "Epoch : 289 / 350, cost : 0.6736942529678345\n",
            "Epoch : 290 / 350, cost : 0.6700453162193298\n",
            "Epoch : 291 / 350, cost : 0.6740367412567139\n",
            "Epoch : 292 / 350, cost : 0.6698006391525269\n",
            "Epoch : 293 / 350, cost : 0.6672906279563904\n",
            "Epoch : 294 / 350, cost : 0.6609886884689331\n",
            "Epoch : 295 / 350, cost : 0.6649600863456726\n",
            "Epoch : 296 / 350, cost : 0.6661123633384705\n",
            "Epoch : 297 / 350, cost : 0.6568256616592407\n",
            "Epoch : 298 / 350, cost : 0.6579506993293762\n",
            "Epoch : 299 / 350, cost : 0.6608540415763855\n",
            "Epoch : 300 / 350, cost : 0.6548657417297363\n",
            "Epoch : 301 / 350, cost : 0.6549775004386902\n",
            "Epoch : 302 / 350, cost : 0.6492154598236084\n",
            "Epoch : 303 / 350, cost : 0.6438223719596863\n",
            "Epoch : 304 / 350, cost : 0.649938702583313\n",
            "Epoch : 305 / 350, cost : 0.649361252784729\n",
            "Epoch : 306 / 350, cost : 0.647744357585907\n",
            "Epoch : 307 / 350, cost : 0.6465912461280823\n",
            "Epoch : 308 / 350, cost : 0.645114541053772\n",
            "Epoch : 309 / 350, cost : 0.6420206427574158\n",
            "Epoch : 310 / 350, cost : 0.6405908465385437\n",
            "Epoch : 311 / 350, cost : 0.6382876634597778\n",
            "Epoch : 312 / 350, cost : 0.6400341987609863\n",
            "Epoch : 313 / 350, cost : 0.6425738334655762\n",
            "Epoch : 314 / 350, cost : 0.6395255923271179\n",
            "Epoch : 315 / 350, cost : 0.639487624168396\n",
            "Epoch : 316 / 350, cost : 0.6295563578605652\n",
            "Epoch : 317 / 350, cost : 0.6231483817100525\n",
            "Epoch : 318 / 350, cost : 0.621547520160675\n",
            "Epoch : 319 / 350, cost : 0.6302590370178223\n",
            "Epoch : 320 / 350, cost : 0.6155209541320801\n",
            "Epoch : 321 / 350, cost : 0.6221886873245239\n",
            "Epoch : 322 / 350, cost : 0.6188154220581055\n",
            "Epoch : 323 / 350, cost : 0.6179015040397644\n",
            "Epoch : 324 / 350, cost : 0.6166080832481384\n",
            "Epoch : 325 / 350, cost : 0.6201546788215637\n",
            "Epoch : 326 / 350, cost : 0.6160887479782104\n",
            "Epoch : 327 / 350, cost : 0.6107373237609863\n",
            "Epoch : 328 / 350, cost : 0.6074569821357727\n",
            "Epoch : 329 / 350, cost : 0.6082298159599304\n",
            "Epoch : 330 / 350, cost : 0.6008768677711487\n",
            "Epoch : 331 / 350, cost : 0.598625898361206\n",
            "Epoch : 332 / 350, cost : 0.603259265422821\n",
            "Epoch : 333 / 350, cost : 0.5974910855293274\n",
            "Epoch : 334 / 350, cost : 0.5944112539291382\n",
            "Epoch : 335 / 350, cost : 0.5937541127204895\n",
            "Epoch : 336 / 350, cost : 0.5973309874534607\n",
            "Epoch : 337 / 350, cost : 0.5884706974029541\n",
            "Epoch : 338 / 350, cost : 0.5904760360717773\n",
            "Epoch : 339 / 350, cost : 0.5889044404029846\n",
            "Epoch : 340 / 350, cost : 0.5896430611610413\n",
            "Epoch : 341 / 350, cost : 0.5943700671195984\n",
            "Epoch : 342 / 350, cost : 0.5897652506828308\n",
            "Epoch : 343 / 350, cost : 0.5865374207496643\n",
            "Epoch : 344 / 350, cost : 0.58167564868927\n",
            "Epoch : 345 / 350, cost : 0.5824709534645081\n",
            "Epoch : 346 / 350, cost : 0.5811342597007751\n",
            "Epoch : 347 / 350, cost : 0.578377366065979\n",
            "Epoch : 348 / 350, cost : 0.5848076939582825\n",
            "Epoch : 349 / 350, cost : 0.5833045840263367\n",
            "Epoch : 350 / 350, cost : 0.5786299109458923\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 미리 작성된 코드들은 수정할 수 없으며, 이외의 코드를 작성하시면 됩니다.\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    # 모델의 코드는 여기서 작성해주세요\n",
        "\n",
        "    def __init__(self, drop_prob):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(32*32*3, 2048) # input size : 32*32*3 (RGB)\n",
        "        self.linear2 = nn.Linear(2048, 1024)\n",
        "        self.linear3 = nn.Linear(1024, 512)\n",
        "        self.linear4 = nn.Linear(512, 10)       # output size : 10\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.linear1(x)\n",
        "        a1 = self.activation(z1)\n",
        "        a1 = self.dropout(a1)\n",
        "\n",
        "        z2 = self.linear2(a1)\n",
        "        a2 = self.activation(z2)\n",
        "        a2 = self.dropout(a2)\n",
        "        \n",
        "        z3 = self.linear3(a2)\n",
        "        a3 = self.activation(z3)\n",
        "        a3 = self.dropout(a3)\n",
        "        \n",
        "        output = self.linear4(a3)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 학습코드는 모두 여기서 작성해주세요\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                                 train=True,\n",
        "                                                 transform=transforms.ToTensor(),\n",
        "                                                 download=True)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                                train=False,\n",
        "                                                transform=transforms.ToTensor(),\n",
        "                                                download=True)\n",
        "    \n",
        "    batch_size = 4096\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = Classifier(0.5).to(device)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.95, 0.999)) \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    epochs = 350\n",
        "    lmbd = 0.003\n",
        "\n",
        "    total_batch_num = len(train_dataloader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      avg_cost = 0\n",
        "      model.train()\n",
        "\n",
        "      for b_x, b_y in train_dataloader:\n",
        "        b_x = b_x.view(-1, 32*32*3).to(device)\n",
        "        logits = model(b_x) # forward prop\n",
        "        loss = criterion(logits, b_y.to(device)) # get cost\n",
        "\n",
        "        reg = model.linear1.weight.pow(2.0).sum()\n",
        "        reg += model.linear2.weight.pow(2.0).sum()\n",
        "        reg += model.linear3.weight.pow(2.0).sum()\n",
        "\n",
        "        loss += lmbd*reg/len(b_x)/len(b_x)/2.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # backward prop\n",
        "        optimizer.step() # update parameters\n",
        "\n",
        "        avg_cost += loss / total_batch_num # 모든 데이터셋에 대한 cost 값\n",
        "\n",
        "      print('Epoch : {} / {}, cost : {}'.format(epoch+1, epochs, avg_cost))\n",
        "\n",
        "    torch.save(model.state_dict(), 'model.pt')  # 학습된 모델을 저장하는 코드입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델의 성능을 평가하는 코드입니다.\n",
        "# 아래의 코드로 평가를 진행할 예정이므로 아래의 코드가 정상 동작 해야하며, 제출전 모델의 성능을 확인하시면 됩니다.\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                            train=False,\n",
        "                                            transform=transforms.ToTensor(),\n",
        "                                            download=True)\n",
        "\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10000)\n",
        "\n",
        "classifier = Classifier(0.5).to(device)\n",
        "classifier.load_state_dict(torch.load('model.pt'))\n",
        "classifier.eval()\n",
        "\n",
        "\n",
        "for data, label in test_dataloader:\n",
        "    data = data.view(-1, 32 * 32 * 3).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = classifier(data)\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        total = len(label)\n",
        "        correct = torch.eq(pred, label.to(device)).sum()\n",
        "\n",
        "        print(\"Accuracy on test set : {:.4f}%\".format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quViAYFRryUx",
        "outputId": "82d938b7-e5fd-4349-de91-b09394e57523"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test set : 58.6700%\n"
          ]
        }
      ]
    }
  ]
}